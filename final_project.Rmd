---
output:
  html_document: default
---

# Final Project

For this tutorial, we will be using data collected on my personal eating habits and logged on [MyFitnessPal.com](https://www.myfitnesspal.com/). MyFitnessPal is a website and mobile application which lets you enter in food you have eaten in a day and grabs their nutritional information from a database. I have been logging my eating habits on MyFitnessPal for about two months, and we will collect and analyze this data.  
This analysis will be particularly interesting as it uses personal data. Using what we learn from this exercise, we can learn about our own eating habits and maybe even see what influences our eating choices.

### Collecting and Ingesting Data

After eating habits have been logged for the desired period of analysis, it is time to collect your data. If you own a premium account in MyFitnessPal, you may download the full report directly as a CSV file, which is a much neater way of doing this. In this tutorial, however, we will instead take the data directly from the printout generated by MyFitnessPal. If the table MyFitnessPal generated was publicly available, it would be easy to use R to scrape the data directly from a URL, but instead we will use the following process.  

First, you must go to the "Food" tab at MyFitnessPal.com, scroll to the bottom, and click "View Full Report (Printable)." Set the time period in which you want to analyze your data and uncheck all boxes except for "Food Diary," then click "change report." You should now see a page with all of your logged food, the days the food was eaten, the nutritional information of each food, etc. Starting at the first date printed, highlight everything until the very end of the page. Copy and paste this into an excel file and save as a Comma Delimited file, "MyFitnessPalData.csv". After this step, your file, when opened with Excel, should look something like this:  

![](C:\Users\Nate\Documents\College\Sophomore\2019 Spring\CMSC 320\Final Project\FitnessDataCSV.png)  

To make the table a bit easier to work with later, insert a row at the very top, copying the data from row 2. The top row should now say "FOODS | Calories | ... | Sugars | Fiber".  

Now, we must load our CSV into R.
  
```{r load_data, message=FALSE, warning=FALSE}
library(dplyr)
library(tidyverse)
library(ggplot2)

food_tab <- read_csv("MyFitnessPalData.csv")
food_tab
```
  
### Tidying Data

  We now have our table loaded into R, but it doesn't make much sense right now for our purposes. We want our data to be tidy and conform to the ER Model, in which:  
  
  1. Each attribute forms a column  
  2. Each entity forms a row  
  3. Each type of entity forms a table  
  
You can read more about tidy data [here.](http://www.hcbravo.org/IntroDataSci/bookdown-notes/tidying-data.html)  
Right now, our data is not tidy. we have dates and meal types being treated as entities when they should be attributes, we have quanities of foods mixed in with foods' names, our column headers are incorrect, and we have units included in measurements, among other issues, which would all make calculations more difficult. To tidy our data, we must fix these things before any analysis can occur.  
  
First, we will rename our "FOODS" column to match the style of the rest of the columns. We will also remove the entities that are not useful to us, which are those that simply list the nutrient types and those giving daily totals (we can calculate daily totals in our analysis later).  
```{r}
# Rename FOODS to Foods
tidy_food <- food_tab %>%
  rename("Foods"="FOODS")

# Remove Excess Entities
tidy_food <- tidy_food[!(tidy_food$Foods=="FOODS" | tidy_food$Foods=="TOTAL:"),]
tidy_food
```

Next, we will convert each date from an entity to an attribute called "Date" for each corresponding food and then remove date entities. Additionally, we will convert our new "Date" column into a date type.

```{r}
# 
curr_date <- "__-___-__"
for (row in 1:nrow(tidy_food)) {
  potential_date <- tidy_food[row, "Foods"]
  if (grepl("\\d{1,}-\\D{3}-\\d{2}", potential_date)) {
    curr_date <- potential_date
  } else {
    tidy_food[row, "Date"] = curr_date
  }
}
tidy_food <- tidy_food[!(grepl("\\d{1,}-\\D{3}-\\d{2}", tidy_food$Foods)),]
tidy_food$Date <- as.Date(tidy_food$Date, "%d-%b-%y")
tidy_food
```

Now, we will do the same thing with the type of meal, creating a column called "Meal".

```{r}
meals = c("Breakfast", "Lunch", "Dinner", "Snacks")
curr_meal <- "___"
for (row in 1:nrow(tidy_food)) {
  potential_meal <- tidy_food[row, "Foods"]
  if (potential_meal %in% meals) {
    curr_meal <- potential_meal
  } else {
    tidy_food[row, "Meal"] = curr_meal
  }
}
tidy_food <- tidy_food[!(tidy_food$Foods %in% meals),]
tidy_food
```

Finally, we split our Foods column into two parts: Food and Quantity.
```{r, warning=FALSE}
tidy_food <- tidy_food %>%
  separate("Foods", c("Food", "Quantity"), sep=", (?=\\d)")
tidy_food
```

After this, our data conforms to the ER Model:  
  
  1. Each attribute forms a column  
  2. Each entity forms a row  
  3. Each type of entity forms a table  

Now, just to make our nutrient attributes easier to use in calculations, we will replace all missing nutrient values with 0 and convert each integer column to be an integer type.
```{r}
# Replace missing nutrients with 0
tidy_food$Carbs <- replace(tidy_food$Carbs, tidy_food$Carbs=="#NAME?", "0g")
tidy_food$Fat <- replace(tidy_food$Fat, tidy_food$Fat=="#NAME?", "0g")
tidy_food$Protein <- replace(tidy_food$Protein, tidy_food$Protein=="#NAME?", "0g")
tidy_food$Cholest <- replace(tidy_food$Cholest, tidy_food$Cholest=="#NAME?", "0mg")
tidy_food$Sodium <- replace(tidy_food$Sodium, tidy_food$Sodium=="#NAME?", "0mg")
tidy_food$Sugars <- replace(tidy_food$Sugars, tidy_food$Sugars=="#NAME?", "0g")
tidy_food$Fiber <- replace(tidy_food$Fiber, tidy_food$Fiber=="#NAME?", "0g")

# Remove commas and units from nutrient amounts
tidy_food <- tidy_food %>%
  mutate(Calories = (gsub(",", "", Calories))) %>%
  mutate(Carbs = (gsub(",", "", Carbs))) %>%
  mutate(Fat = (gsub(",", "", Fat))) %>%
  mutate(Protein = (gsub(",", "", Protein))) %>%
  mutate(Cholest = (gsub(",", "", Cholest))) %>%
  mutate(Sodium = (gsub(",", "", Sodium))) %>%
  mutate(Sugars = (gsub(",", "", Sugars))) %>%
  mutate(Fiber = (gsub(",", "", Fiber))) %>%
  mutate(Calories = as.integer(Calories)) %>%
  mutate(Carbs = as.integer(gsub("g", "", Carbs))) %>%
  mutate(Fat = as.integer(gsub("g", "", Fat))) %>%
  mutate(Protein = as.integer(gsub("g", "", Protein))) %>%
  mutate(Cholest = as.integer(gsub("mg", "", Cholest))) %>%
  mutate(Sodium = as.integer(gsub("mg", "", Sodium))) %>%
  mutate(Sugars = as.integer(gsub("g", "", Sugars))) %>%
  mutate(Fiber = as.integer(gsub("g", "", Fiber)))
tidy_food
```



### Exploratory Data Analysis

Now that our data has been collected and tidied, it is ready for analysis. First, let's write code to produce a plot that shows daily calories over time. For this purpose, we will use a scatter plot and fit it with a linear regression. To learn about the different types of plots provided in ggplot2, [this cheat sheet](https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf) provides plot names, their parameters, when they are used, and more.

```{r}
# Produces a boxplot for the entire duration of the data which shows the trend of calories across meals over time
tidy_food %>%
  # Group all meals into their respective dates and sum each day's calories
  dplyr::group_by(Date) %>%
  dplyr::summarise(Total_Cals = sum(Calories)) %>%
  # Make plot
  ggplot(mapping = aes(x=Date, y=Total_Cals)) +
  labs(title = "Daily Consumed Calories Over Time", x="Date", y="Calories") +
  geom_point() +
  geom_smooth(method = lm)
```

Based off of this graph, what can we say about the trend in calories consumed conditioned on time? From March 6 to May 1, there was a general downward trend in calories, with the spread being greater in the first and last third or so of time. There were a good deal of outliers, but there was also a good amount of data near the regression line.  
We can also expand our analysis to be based on meal instead of date. Unlike time, which is a continuous measurement, the type of meal (breakfast, lunch, dinner, or snacks) is a discrete value, so a scatter plot doesn't make as much sense. To show the distribution of calories for different meals for the entire time period, we will use a box plot.

```{r}
# Produces a boxplot for the entire duration of the data which shows the trend of nutrients across meals over time
tidy_food %>%
  # Group data by meal in which it was consumed
  dplyr::group_by(Date, Meal) %>%
  dplyr::summarise(Total_Cals = sum(Calories)) %>%
  # Make plot
  ggplot(mapping = aes(x=Meal, y=Total_Cals)) +
  labs(title = "Distribution of Calories Consumed in Each Meal", x="Meal", y="Calories") +
  geom_boxplot()
```

Based on this plot, we can see that generally, breakfast and snacks have the least calories, while lunch has more, and dinner has the most. Dinner, lunch, and snacks all have upper outliers, which makes sense. Going out to eat tends to cause a greater consumption of calories, and you are more likely to go out to eat for lunch or dinner than breakfast. Snacks, as well, tend to have a greater consumption at things like parties.  

What if, however, we wanted to see how calorie consumption in each meal changes over time?

```{r}
# Produces a boxplot for the entire duration of the data which shows the trend of calories across meals over time
tidy_food %>%
  # Group all meals into their respective dates and sum each meal's calories
  dplyr::group_by(Date, Meal) %>%
  dplyr::summarise(Total_Cals = sum(Calories)) %>%
  # Make plot
  ggplot(mapping = aes(x=Date, y=Total_Cals, colour = Meal)) +
  labs(title = "Daily Consumed Calories Over Time", x="Date", y="Calories") +
  geom_point() +
  geom_smooth(method = lm, se=FALSE)
```
We will use these two plots later in our regression testing.

### Hypothesis Testing and Machine Learning

Suppose we wanted to test if date had a significant impact on calories consumed. To do this, we could fit a linear regression model of calories vs. date (treated as a continuous variable), and test for a relationship between date and calories. We can hypothesize that there is a relationship, so the null hypothesis would be that there is no relationship. can we reject the null hypothesis?  
To determine this, we use a linear regression model for calories vs. year.

```{r}
date_cal_meal <- tidy_food %>%
  # Group all meals into their respective dates and sum each day's calories
  dplyr::group_by(Date, Meal) %>%
  dplyr::summarise(Total_Cals = sum(Calories))

lm_food <- lm(Total_Cals ~ Date, data=date_cal_meal)
broom::tidy(lm_food)
```
This tells us that, on average, total calories consumed in a meal decreases by approximately 1.952772 calories each day.  
The p-value helps us determine the significance of our results. Since the p-value is 0.15, which is greater than 0.05, we cannot reject the null hypothesis of no relationship between date and daily consumed calories when using a 95% confidence interval. However, for the sake of the tutorial, we will continue as if our p-value was within the confidence interval.  
To learn more about hypothesis testing, check out (this website.)[http://www.r-tutor.com/elementary-statistics/hypothesis-testing]  

Ideally, the residuals should be relatively equally scattered and not show any sort of trend over time. However, as we will see in a moment, many of the residuals are close below 0, while a few are far above 0.

```{r}
aug_food <- lm_food %>%
  broom::augment()
aug_food %>%
  ggplot(aes(x=factor(Date), y=.resid)) +
    geom_point() +
    labs(title="Calorie Residuals Over Time",
         x = "Date",
         y = "Residuals")
```

Because of this trend in residuals, we know that there are other factors coming into play when determining the amount of calories consumed.

```{r}
lm_food <- lm(Total_Cals ~ Date, data=date_cal_meal)
aug_food <- lm_food %>%
  broom::augment()

aug_food %>%
  ggplot(aes(x=date_cal_meal$Meal, y=.resid)) +
    geom_boxplot() +
    labs(title="Calorie Residuals Across Meals",
         x = "Meal",
         y = "Residuals")
```

This tells us that there is a dependence between model residual and meal tpye. Because of this, when performing a regression analysis of calories across time, meal type should be included in the model.

```{r}
date_cal_meal %>%
  group_by(Meal) %>%
  ggplot(aes(x=Date, y=Total_Cals, colour = Meal)) +
    geom_smooth(method=lm, se = FALSE) +
    geom_point() +
    labs(title="Calories Consumed Over Time",
         x = "Date",
         y = "Calories")
```

Based on this plot, it seems that meal type has an impact on calories consumed as well as date. Because of this, when performing a regression analysis of calories, meal type should be included in the model in addition to date. Therefore, we will fit a new linear regression model.

```{r}
new_lm_food <- lm(Total_Cals ~ Date*Meal, data=date_cal_meal)
tidy_lm_food <- broom::tidy(new_lm_food)
tidy_lm_food
```

However, it appears that the parameters in this model are not significantly differenct than zero. Additionally, our p-values are high, so we probably have not chosen the right data to condition our analysis on. Still, we can perform an F-test that compares how well the two models fit the data.

```{r}
new_lm_food <- lm(Total_Cals ~ Date:Meal, data=date_cal_meal)
anova(lm_food, new_lm_food, test="F")
```

Though neither model is particularly good, the interaction model is significantly better than the date-only model because it has reduced the residual sum of squares to a significant amount. It is known to be segnificant because the p-value of the test is 1.066e-06, which is much less than $\alpha=0.5$.  
We now make a residuals vs. date plot for the interaction model.

```{r}
aug_food <- new_lm_food %>%
  broom::augment()
aug_food %>%
  ggplot(aes(x=factor(Date), y=.resid)) + 
  geom_point() +
    labs(title="Calorie Residuals Over Time",
         x = "Date",
         y = "Residuals")
```

Though it is far from perfect, the interaction model does a much better job for residuals vs. date as the residuals are more well-distributed, showing less of a trend over time than the initial model. If you want more information on machine learning in R, click [here.](https://www.datacamp.com/community/tutorials/machine-learning-in-r)  
  
In creating these models, we have learned that the date and the type of meal probably influence how many calories will be consumed in a single meal. Though we didn't find a very accurate model, there is a strong chance that trends can be identified using data that perhaps wasn't contained in this table. For example: how much exercise did I get that day? How stressed was I that day? How much food did I have at easy access? Did I go out to eat? All of these are sontributing factors in finding calorie per meal, and all of these are data which we do not have easy access to. However, we now have an insight as to what may be influencing food choices and meal sizes, and perhaps this cacn have an effect on our food choices in the future. This analysis could also be applied to the other nutrients in the data set besides calories, and perhaps we would find more clear trends. Such trends might include finding a correlation between fat and calories or fat and sodium.

